# SLURM Scripts - Updated Summary

All SLURM scripts now follow consistent format with:
- Proper environment setup (CONDA_PREFIX, Singularity paths)
- Singularity container execution
- Email notifications to go2432@wayne.edu
- Consistent bind mount patterns

## Container Requirements

### 1. spineps-preprocessing.sif
- **Used by**: 00_download_data.sh, 01_dicom_to_nifti.sh, 04_lstv_detection.sh
- **Image**: docker://go2432/spineps-preprocessing:latest
- **Contains**: Python, dcm2niix, numpy, pandas, nibabel, kaggle CLI
- **Purpose**: Data download, DICOM conversion, basic Python analysis

### 2. spineps-segmentation.sif
- **Used by**: 02_spineps.sh
- **Image**: docker://go2432/spineps-segmentation:latest
- **Contains**: SPINEPS models, CUDA support, deep learning stack
- **Purpose**: Vertebra and subregion segmentation
- **GPU**: Required (--nv flag)

### 3. totalsegmentator.sif
- **Used by**: 03_totalspineseg.sh
- **Image**: docker://wasserth/totalsegmentator:latest
- **Contains**: TotalSegmentator models, nnU-Net
- **Purpose**: Vertebra segmentation (sagittal + axial)
- **GPU**: Required (--nv flag)

## File Paths

All scripts use consistent path mapping:

```bash
Host                              Container
====================================================================================================
$PROJECT_DIR                   â†’ /work
data/raw/train_images          â†’ /data/input
results/nifti                  â†’ /data/nifti
results/spineps                â†’ /data/output (SPINEPS)
results/totalspineseg          â†’ /data/output (TotalSpineSeg)
results/lstv_detection         â†’ /data/output (LSTV)
models/spineps_cache           â†’ /app/models
models/spineps_pkg_models      â†’ /opt/conda/lib/.../spineps/models
```

## Key Changes from Original

### 00_download_data.sh
- âœ… Kept original format (already correct)
- âŒ Removed Ian Pan model download
- âŒ Removed validation filter - downloads ALL studies

### 01_dicom_to_nifti.sh
- âœ… Added Singularity execution
- âœ… Uses spineps-preprocessing container
- âœ… Maps paths: /data/input, /data/output, /data/raw
- â° Extended time: 4h â†’ 8h

### 02_spineps.sh
- âœ… Already using Singularity (kept existing format)
- âœ… Updated job name: spineps â†’ spineps_run
- â° Extended time: 24h â†’ 36h
- âœ… Matches user's existing format exactly

### 03_totalspineseg.sh
- âœ… Complete rewrite with Singularity
- âœ… Uses TotalSegmentator container
- âœ… Maps paths: /data/nifti, /data/output
- â° Extended time: 24h â†’ 36h
- ğŸ”§ SERIES variable support (sagittal, axial, both)

### 04_lstv_detection.sh
- âœ… Added Singularity execution
- âœ… Uses spineps-preprocessing container (has numpy/nibabel)
- âœ… Maps paths: /data/spineps, /data/totalspine, /data/output
- â° Extended time: 1h â†’ 2h

### 00_run_all.sh
- âœ… Added environment setup (Singularity paths)
- âœ… Passes MODE variable to child jobs
- âœ… Monitors all jobs and reports status
- â° 96h limit (enough for full pipeline)

## Usage

### Quick Start
```bash
cd ~/lstv-detector

# Copy all updated scripts
cp /path/to/outputs/*.sh slurm_scripts/
chmod +x slurm_scripts/*.sh

# Run trial mode (3 studies)
MODE=trial sbatch slurm_scripts/00_run_all.sh

# Run production (all studies)
MODE=prod sbatch slurm_scripts/00_run_all.sh
```

### Manual Execution
```bash
# If data already exists
MODE=prod sbatch slurm_scripts/01_dicom_to_nifti.sh
# Wait, then:
MODE=prod sbatch slurm_scripts/02_spineps.sh
MODE=prod sbatch slurm_scripts/03_totalspineseg.sh
# Wait, then:
sbatch slurm_scripts/04_lstv_detection.sh
```

### Monitor Jobs
```bash
squeue -u $USER
tail -f logs/lstv_pipeline_*.out
ls -ltr logs/
```

## Expected Timeline

| Step | Duration | Resource |
|------|----------|----------|
| 0. Download | ~2-4h | CPU |
| 1. DICOMâ†’NIfTI | ~4-8h | CPU |
| 2. SPINEPS | ~24-36h | GPU (parallel) |
| 3. TotalSpineSeg | ~24-36h | GPU (parallel) |
| 4. LSTV Detection | ~0.5-2h | CPU |
| **Total** | **~24-48h** | (with GPU availability) |

## Outputs

```
results/
â”œâ”€â”€ nifti/
â”‚   â”œâ”€â”€ {study_id}_sag_t2.nii.gz
â”‚   â””â”€â”€ {study_id}_axial_t2.nii.gz
â”œâ”€â”€ spineps/
â”‚   â””â”€â”€ segmentations/
â”‚       â”œâ”€â”€ {study_id}_seg-vert_msk.nii.gz
â”‚       â”œâ”€â”€ {study_id}_seg-spine_msk.nii.gz
â”‚       â”œâ”€â”€ {study_id}_ctd.json (ALL structures!)
â”‚       â””â”€â”€ {study_id}_unc.nii.gz
â”œâ”€â”€ totalspineseg/
â”‚   â”œâ”€â”€ {study_id}_sagittal_vertebrae.nii.gz
â”‚   â””â”€â”€ {study_id}_axial_vertebrae.nii.gz
â””â”€â”€ lstv_detection/
    â”œâ”€â”€ lstv_results.json
    â””â”€â”€ lstv_summary.json
```

## Troubleshooting

### Container not found
```bash
ls ~/singularity_cache/
# If missing, scripts will auto-pull on first run
```

### GPU not available
```bash
sinfo -o "%20N %10c %10m %25f %10G"
# Check v100 GPU availability
```

### Bind mount errors
```bash
# Ensure directories exist before job runs
mkdir -p results/nifti results/spineps results/totalspineseg
mkdir -p models/spineps_cache models/spineps_pkg_models
```

### Python script not found
```bash
# Ensure scripts are in PROJECT_DIR/scripts/
ls scripts/*.py
```

## Email Notifications

All scripts send email to: **go2432@wayne.edu**
- BEGIN: When job starts
- END: When job completes successfully
- FAIL: When job fails

Update email in each script header if needed:
```bash
#SBATCH --mail-user=YOUR_EMAIL@wayne.edu
```
